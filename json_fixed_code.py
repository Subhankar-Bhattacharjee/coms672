import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

from dotenv import load_dotenv, find_dotenv
import nest_asyncio
import asyncio
import json
import requests
import ast
import re
import inspect

_ = load_dotenv(find_dotenv())
# Load API key directly
MISTRAL_API_KEY = "API KEY"
nest_asyncio.apply()


test_prompt = """You are an expert Python code reviewer specializing in bug detection and correction validation. Your task is to rigorously evaluate whether proposed patches correctly fix bugs in Python programs.

You will be provided with three code versions for each program:
1. BUGGY CODE: The original program containing one or more bugs
2. GROUND TRUTH: The official correct implementation that properly fixes all bugs
3. PATCH TO EVALUATE: A solution generated by an LLM that attempts to fix the bugs

Evaluation criteria:
- Correctness: Does the patch fix ALL bugs present in the buggy code?
- Equivalence: Is the patch functionally equivalent to the ground truth?
- Efficiency: Is the patch solution as efficient as the ground truth?
- Readability: Is the code clean, well-documented, and following good practices?

For your evaluation:
- Focus on semantic correctness over stylistic differences
- Identify any bugs that remain unfixed
- Note any new bugs introduced by the patch
- Compare algorithmic approaches between patch and ground truth
- Assess edge case handling

Return a JSON object with these fields:
- "is_correct": Boolean (true if patch correctly fixes ALL bugs)
- "explanation": Detailed analysis of your evaluation
- "score": Integer from 0-10 (0=completely incorrect, 10=perfect match to ground truth)
- "unfixed_bugs": Array of any bugs that remain unfixed
- "introduced_bugs": Array of any new bugs introduced
- "performance_assessment": Brief assessment of efficiency compared to ground truth
- "test_cases": Generate at least 3 meaningful test cases as JSON objects with input, expected output, and whether the patch would pass these tests
"""

# Custom JSON encoder to handle non-JSON serializable objects
class CustomJSONEncoder(json.JSONEncoder):
    def default(self, obj):
        # Convert sets to lists
        if isinstance(obj, set):
            return list(obj)
        # Convert any generators to lists
        elif hasattr(obj, '__iter__') and not isinstance(obj, (str, list, dict, tuple)):
            return list(obj)
        # Convert any custom objects to their string representation
        elif hasattr(obj, '__dict__'):
            return str(obj)
        # Let the base class handle everything else
        return super().default(obj)

# Function to read files from directory
def read_files_from_directory(directory_path):
    files_content = {}
    # Check if directory exists
    if not os.path.exists(directory_path):
        print(f"Warning: Directory {directory_path} does not exist.")
        return files_content
        
    # Check if directory is empty
    if not os.listdir(directory_path):
        print(f"Warning: Directory {directory_path} is empty.")
        return files_content
        
    for filename in os.listdir(directory_path):
        if filename.endswith('.py'):
            file_path = os.path.join(directory_path, filename)
            try:
                with open(file_path, 'r') as file:
                    files_content[filename] = file.read()
            except Exception as e:
                print(f"Error reading file {file_path}: {e}")
    return files_content

# Function to call Mistral API
async def call_mistral_api(system_message, user_message):
    url = "https://api.mistral.ai/v1/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {MISTRAL_API_KEY}"
    }
    data = {
        "model": "mistral-large-latest",  # Using the latest large model
        "messages": [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ],
        "temperature": 0.2,
        "response_format": {"type": "json_object"}  # Request JSON output
    }
    
    print(f"Sending request to Mistral API...")
    try:
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()  # Raise exception for HTTP errors
        print(f"Response status: {response.status_code}")
        response_json = response.json()
        return response_json["choices"][0]["message"]["content"]
    except requests.exceptions.RequestException as e:
        print(f"Error with Mistral API request: {e}")
        if 'response' in locals() and hasattr(response, 'text'):
            print(f"Response details: {response.text}")
        return None

# Function to parse a Python function and extract information
def analyze_function(code_str):
    try:
        # Parse the code into an AST
        tree = ast.parse(code_str)
        
        # Find the function definition
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                func_name = node.name
                
                # Extract parameters
                params = [arg.arg for arg in node.args.args]
                
                # Extract docstring if it exists
                docstring = ast.get_docstring(node)
                
                return {
                    "function_name": func_name,
                    "parameters": params,
                    "docstring": docstring
                }
        
        return {"error": "No function definition found"}
    except Exception as e:
        return {"error": f"Failed to parse code: {str(e)}"}

# Function to extract test cases from the function
def extract_test_cases(func_info, code_str):
    # Try to execute the code to get function object
    function_name = func_info.get("function_name")
    if not function_name:
        return []
    
    # Create some test inputs based on parameter types and function name
    params = func_info.get("parameters", [])
    test_cases = []
    
    # Generate some meaningful test cases based on common function naming patterns
    if "sum" in function_name.lower():
        test_cases.append({"input": [1, 2, 3], "description": "Sum of positive integers"})
        test_cases.append({"input": [-1, -2, -3], "description": "Sum of negative integers"})
        test_cases.append({"input": [], "description": "Empty list"})
    elif "sort" in function_name.lower():
        test_cases.append({"input": [3, 1, 4, 1, 5], "description": "Random integers"})
        test_cases.append({"input": [1, 2, 3, 4, 5], "description": "Already sorted"})
        test_cases.append({"input": [5, 4, 3, 2, 1], "description": "Reverse sorted"})
    elif "count" in function_name.lower():
        test_cases.append({"input": 42, "description": "Positive integer"})
        test_cases.append({"input": 0, "description": "Zero"})
        test_cases.append({"input": -42, "description": "Negative integer"})
    else:
        # Generate some generic test cases
        if len(params) == 1:
            test_cases.append({"input": 42, "description": "Positive integer"})
            test_cases.append({"input": 0, "description": "Zero"})
            test_cases.append({"input": -42, "description": "Negative integer"})
        elif len(params) > 1:
            test_cases.append({"input": [1, 2], "description": "Two positive integers"})
            test_cases.append({"input": [0, 0], "description": "Two zeros"})
            test_cases.append({"input": [-1, -2], "description": "Two negative integers"})
    
    return test_cases

# Function to make an object JSON serializable
def make_serializable(obj):
    """Recursively convert objects to JSON serializable types."""
    if isinstance(obj, dict):
        return {k: make_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list) or isinstance(obj, tuple):
        return [make_serializable(item) for item in obj]
    elif isinstance(obj, set):
        return [make_serializable(item) for item in obj]
    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, list, dict, tuple)):
        # Handle generators and other iterables
        return [make_serializable(item) for item in obj]
    elif hasattr(obj, '__dict__'):
        # Handle custom objects
        return str(obj)
    else:
        # Replace non-serializable objects with their string representation
        try:
            json.dumps(obj)
            return obj
        except (TypeError, OverflowError):
            return str(obj)

# Function to run a test case through both implementations
def run_test_case(func_code, test_input, is_ground_truth=False):
    # Create a temporary environment to execute the function
    env = {}
    try:
        # Execute the function code in the environment
        exec(func_code, env)
        
        # Get the function object
        func_name = next((name for name in env if callable(env[name])), None)
        
        if not func_name:
            return {"error": "No callable function found in the code"}
        
        func = env[func_name]
        
        # Check if test_input is a list or single value based on function signature
        sig = inspect.signature(func)
        
        # If test_input is a list but function takes single parameter, adjust accordingly
        if isinstance(test_input, list) and len(sig.parameters) == 1:
            result = func(test_input)
        # If test_input is a single value but function takes multiple parameters, adjust accordingly
        elif not isinstance(test_input, list) and len(sig.parameters) > 1:
            # Create a list of the same value for each parameter
            input_list = [test_input] * len(sig.parameters)
            result = func(*input_list)
        # If function takes multiple parameters and we have a list of inputs
        elif isinstance(test_input, list) and len(sig.parameters) > 1:
            # Pad or truncate input list as needed
            padded_input = test_input[:len(sig.parameters)]
            if len(padded_input) < len(sig.parameters):
                padded_input.extend([None] * (len(sig.parameters) - len(padded_input)))
            result = func(*padded_input)
        else:
            # Single parameter, single input
            result = func(test_input)
        
        # Make sure the result is JSON serializable
        result = make_serializable(result)
        
        return {
            "result": result,
            "status": "success",
            "implementation": "ground_truth" if is_ground_truth else "patch"
        }
    except Exception as e:
        return {
            "error": str(e),
            "status": "error",
            "implementation": "ground_truth" if is_ground_truth else "patch"
        }

# Function to evaluate a single patch with test cases
async def evaluate_patch(buggy_code, ground_truth, patch, use_mistral=True):
    system_message = test_prompt
    
    user_message = f"""
    Buggy Code:
    ```python
    {buggy_code}
    ```
    
    Ground Truth (Correct Fix):
    ```python
    {ground_truth}
    ```
    
    Patch to Evaluate:
    ```python
    {patch}
    ```
    
    Please evaluate if the patch correctly fixes the bug in the code. 
    Return your answer as a JSON with the following fields:
    - "is_correct": boolean indicating if the patch correctly fixes the bug
    - "explanation": detailed explanation of your evaluation
    - "score": a score from 0-10 based on how close the patch is to the ground truth
    - "unfixed_bugs": array of any bugs that remain unfixed
    - "introduced_bugs": array of any new bugs introduced
    - "performance_assessment": brief assessment of efficiency compared to ground truth
    - "test_cases": array of at least 3 test cases with these fields:
      - "input": the input value(s) for the test case (as a JSON value)
      - "expected_output": what the function should return
      - "description": brief description of what the test case is checking
    
    IMPORTANT: Format your response as valid JSON only, with no additional text before or after.
    Ensure the test cases are meaningful and would actually help validate the function.
    """
    
    try:
        if use_mistral:
            response_content = await call_mistral_api(system_message, user_message)
            if response_content is None:
                print("Falling back to OpenAI since Mistral API failed")
                use_mistral = False
        
        if not use_mistral:
            # Fallback to OpenAI
            from openai import OpenAI
            client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY', ''))
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": user_message}
                ],
                temperature=0.2,
                max_tokens=1000,
                response_format={"type": "json_object"}
            )
            response_content = response.choices[0].message.content
        
        try:
            if response_content and response_content.strip():
                llm_evaluation = json.loads(response_content)
                
                # Analyze the function to understand its structure
                func_info = analyze_function(patch)
                
                # Extract test cases from LLM-provided test cases, or generate if missing
                test_cases = llm_evaluation.get("test_cases", [])
                if not test_cases:
                    print("No test cases provided by LLM, generating some...")
                    test_cases = extract_test_cases(func_info, patch)
                
                # Run the test cases on both implementations
                executed_test_cases = []
                trace_number = 1  # Start trace number
                
                for test_case in test_cases:
                    test_input = test_case.get("input")
                    expected_output = test_case.get("expected_output")
                    description = test_case.get("description", "No description provided")
                    
                    # Run the test against ground truth
                    ground_truth_result = run_test_case(ground_truth, test_input, is_ground_truth=True)
                    
                    # Run the test against the patch
                    patch_result = run_test_case(patch, test_input)
                    
                    # Determine if the test passes
                    # A test passes if both implementations give the same result and no errors
                    passes = (ground_truth_result.get("status") == "success" and 
                             patch_result.get("status") == "success" and
                             ground_truth_result.get("result") == patch_result.get("result"))
                    
                    executed_test_case = {
                        "trace_number": trace_number,
                        "description": description,
                        "input": make_serializable(test_input),
                        "expected_output": make_serializable(expected_output or ground_truth_result.get("result", "Unknown")),
                        "ground_truth_output": make_serializable(ground_truth_result.get("result", "Error")),
                        "patch_output": make_serializable(patch_result.get("result", "Error")),
                        "passes": passes,
                        "ground_truth_error": ground_truth_result.get("error"),
                        "patch_error": patch_result.get("error")
                    }
                    
                    executed_test_cases.append(executed_test_case)
                    trace_number += 1
                
                # Combine the LLM evaluation with our test case results
                combined_result = {
                    **llm_evaluation,
                    "function_info": func_info,
                    "executed_test_cases": executed_test_cases,
                    "tests_passed": sum(1 for test in executed_test_cases if test["passes"]),
                    "total_tests": len(executed_test_cases)
                }
                
                # Make sure the final result is JSON serializable
                combined_result = make_serializable(combined_result)
                
                return combined_result
            else:
                raise ValueError("Empty response received")
        except json.JSONDecodeError as e:
            print(f"JSON parsing error: {e}")
            print(f"Response content: {response_content[:100]}...")
            return {
                "is_correct": False,
                "explanation": f"Failed to parse API response as JSON",
                "score": 0,
                "unfixed_bugs": ["Could not evaluate due to response format error"],
                "introduced_bugs": [],
                "performance_assessment": "Could not evaluate due to response format error"
            }
    except Exception as e:
        print(f"Error during evaluation: {e}")
        return {
            "is_correct": False, 
            "explanation": f"Error: {str(e)}", 
            "score": 0, 
            "unfixed_bugs": [], 
            "introduced_bugs": [], 
            "performance_assessment": "Could not evaluate due to error"
        }

# Function to write individual result files
def write_individual_result_files(results, output_dir="test_results"):
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Write each result to a separate file
    for filename, result in results.items():
        # Create a simplified filename (remove .py extension)
        base_filename = filename.replace('.py', '')
        output_file = os.path.join(output_dir, f"{base_filename}_evaluation.txt")
        
        # Determine if patch is correct or not
        verdict = "TRUE" if result.get("is_correct", False) else "FALSE"
        
        # Get explanation
        explanation = result.get("explanation", "No explanation provided")
        
        # Create content with test case information
        content = f"{verdict}\n\n{explanation}\n\n"
        
        # Add test case information
        content += "Test Cases:\n"
        for test_case in result.get("executed_test_cases", []):
            content += f"Trace #{test_case['trace_number']}: {test_case['description']}\n"
            content += f"Input: {test_case['input']}\n"
            content += f"Expected Output: {test_case['expected_output']}\n"
            content += f"Patch Output: {test_case['patch_output']}\n"
            content += f"Pass/Fail: {'PASS' if test_case['passes'] else 'FAIL'}\n\n"
        
        # Add summary
        content += f"Tests Passed: {result.get('tests_passed', 0)}/{result.get('total_tests', 0)}\n"
        
        # Write to file
        with open(output_file, 'w') as f:
            f.write(content)
        
        print(f"Written evaluation for {filename} to {output_file}")

# Main function to run the evaluation
async def main():
    # Paths to the directories
    buggy_dir = "python_programs"
    ground_truth_dir = "ground_truth"
    patches_dir = "patches"
    
    # Check if directories exist and create them if they don't
    for directory in [buggy_dir, ground_truth_dir, patches_dir]:
        if not os.path.exists(directory):
            print(f"Creating directory: {directory}")
            os.makedirs(directory, exist_ok=True)
    
    # Read all files - with error handling
    try:
        buggy_codes = read_files_from_directory(buggy_dir)
        print(f"Successfully read files from {buggy_dir}")
    except Exception as e:
        print(f"Error reading from {buggy_dir}: {e}")
        buggy_codes = {}
    
    try:
        ground_truths = read_files_from_directory(ground_truth_dir)
        print(f"Successfully read files from {ground_truth_dir}")
    except Exception as e:
        print(f"Error reading from {ground_truth_dir}: {e}")
        ground_truths = {}
    
    # Read patches directly using same filenames as the buggy code
    # This is the key change - no longer looking for _context_patch files
    patches = {}
    try:
        patch_files = read_files_from_directory(patches_dir)
        print(f"Successfully read {len(patch_files)} files from {patches_dir}")
        
        # Use the patch files directly using the same filenames
        patches = patch_files
    except Exception as e:
        print(f"Error reading from {patches_dir}: {e}")
        patches = {}
    
    # Print diagnostic information about files found
    print(f"Found {len(buggy_codes)} files in {buggy_dir}")
    print(f"Found {len(ground_truths)} files in {ground_truth_dir}")
    print(f"Found {len(patches)} patch files in {patches_dir}")
    
    # List all the files found in patches
    print("\nPatch files detected:")
    for key in patches.keys():
        print(f"  - {key}")
    
    # Check which files are missing patches
    print("\nBuggy files missing corresponding patches:")
    for filename in buggy_codes:
        if filename not in patches:
            print(f"  - {filename}")
    
    print("\nUsing Mistral API for evaluation...")
    
    results = {}
    
    # Evaluate each patch
    for filename in buggy_codes:
        if filename in ground_truths and filename in patches:
            print(f"Evaluating {filename}...")
            result = await evaluate_patch(
                buggy_codes[filename],
                ground_truths[filename],
                patches[filename],
                use_mistral=True
            )
            results[filename] = result
        else:
            missing = []
            if filename not in ground_truths:
                missing.append("ground truth")
            if filename not in patches:
                missing.append("patch")
            print(f"Skipping {filename} - missing {', '.join(missing)}")
    
    # Save results to a JSON file, using our custom encoder
    try:
        with open("enhanced_evaluation_results.json", "w") as f:
            json.dump(results, f, indent=4, cls=CustomJSONEncoder)
        print("Successfully saved enhanced_evaluation_results.json")
    except Exception as e:
        print(f"Error saving enhanced_evaluation_results.json: {e}")
        # Try a more aggressive approach to make it JSON serializable
        print("Trying alternative serialization approach...")
        with open("enhanced_evaluation_results.json", "w") as f:
            json.dump(make_serializable(results), f, indent=4)
    
    # Write individual result files
    write_individual_result_files(results)
    
    # Create a summary JSON with the format requested
    summary = []
    for filename, result in results.items():
        for test_case in result.get("executed_test_cases", []):
            summary_entry = {
                "filename": filename,
                "trace_number": test_case["trace_number"],
                "test_case_by_llm": test_case["description"],
                "real_test_case": True,  # Assuming these are real test cases
                "input": test_case["input"],
                "expected_output": test_case["expected_output"],
                "patch_output": test_case["patch_output"],
                "pass_fail": "PASS" if test_case["passes"] else "FAIL"
            }
            summary.append(summary_entry)
    
    # Save the summary to a JSON file, with similar error handling
    try:
        with open("test_summary.json", "w") as f:
            json.dump(summary, f, indent=4, cls=CustomJSONEncoder)
        print("Successfully saved test_summary.json")
    except Exception as e:
        print(f"Error saving test_summary.json: {e}")
        with open("test_summary.json", "w") as f:
            json.dump(make_serializable(summary), f, indent=4)
    
    # Print summary
    total_count = len(results)
    
    print("\nEvaluation complete!")
    if total_count > 0:
        correct_count = sum(1 for result in results.values() if result.get("is_correct", False))
        average_score = sum(result.get("score", 0) for result in results.values()) / total_count
        
        total_tests = sum(result.get("total_tests", 0) for result in results.values())
        passed_tests = sum(result.get("tests_passed", 0) for result in results.values())
        
        print(f"Correct patches: {correct_count}/{total_count} ({correct_count/total_count*100:.2f}%)")
        print(f"Average score: {average_score:.2f}/10")
        print(f"Tests passed: {passed_tests}/{total_tests} ({passed_tests/total_tests*100:.2f}%)")
        print(f"Individual evaluation files have been saved in the 'test_results' folder")
        print(f"Enhanced results saved to 'enhanced_evaluation_results.json'")
        print(f"Test summary saved to 'test_summary.json'")
    else:
        print("No matching files were found across all three directories.")

# Run the main function
if __name__ == "__main__":
    asyncio.run(main())
